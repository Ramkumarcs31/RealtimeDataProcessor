application {
  app-name = "RealtimeDataProcessor"
  zookeeper_host = "10.57.115.116"
  kafka_offset_zookeeper_node = "/requestKafkaOffset"
  sparkbatchinterval = 15
  request-driver-options = "-XX:+UseG1GC -Xms4g -Xmx8g -Dlog4j.configuration=file:/usr/local/spark-1.5.1-bin-hadoop2.6/conf/log4j_RequestLogDriver.properties"
  request-executor-options = "-XX:+UseG1GC -Dlog4j.configuration=file:/usr/local/spark-1.5.1-bin-hadoop2.6/conf/log4j_RequestLogExecutor.properties"
  event-driver-options = "-XX:+UseG1GC -Xms4g -Xmx8g -Dlog4j.configuration=file:/usr/local/spark-1.5.1-bin-hadoop2.6/conf/log4j_EventLogDriver.properties"
  event-executor-options = "-XX:+UseG1GC -Dlog4j.configuration=file:/usr/local/spark-1.5.1-bin-hadoop2.6/conf/log4j_EventLogExecutor.properties"
  spark-streaming=["spark.cores.max=8","spark.serializer=org.apache.spark.serializer.KryoSerializer", "spark.driver.memory=10g", "spark.executor.memory=15g", "spark.storage.memoryFraction=0.5",  "spark.driver.allowMultipleContexts=true", "spark.streaming.blockInterval=400ms", "spark.streaming.concurrentJobs=4", "spark.mesos.coarse=true", "spark.streaming.unpersist=true", "spark.sql.tungsten.enabled=true", "spark.sql.codegen=false", "spark.sql.unsafe.enabled=false" , "spark.streaming.kafka.maxRatePerPartition=10",  "spark.streaming.stopGracefullyOnShutdown=true"]
  spark-batch=["spark.executor.instances-10", "spark.executor.memory-6g", "spark.executor.userClassPathFirst-true", "spark.driver.userClassPathFirst-true"]

  zookeeper_host = "52.15.35.223"
  kafka_offset_zookeeper_node = "/KafkaOffset"
}